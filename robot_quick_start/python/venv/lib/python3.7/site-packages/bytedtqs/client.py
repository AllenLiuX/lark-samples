# coding: utf-8
from __future__ import absolute_import, division, print_function, unicode_literals

import csv
import json
import logging
import os
import random
import sys
import time
from typing import Any, Dict, Iterator, List, Text

import bytedenv
import requests
import servicediscovery
import six

from .clusters import CLUSTER_MAP, Cluster
from .entities import AnalysisResultEntity, JobEntity, QueryResultEntity
from .errors import TQSAuthException, TQSEmptyConsulResultException
from .utils import is_ipv6, split_query_conf, get_local_ip, get_version_number

logger = logging.getLogger("bytedtqs")
if len(logger.handlers) == 0:
    log_handler = logging.StreamHandler(sys.stdout)
    log_handler.setFormatter(
        logging.Formatter("[%(asctime)s] - [%(levelname)s] - %(message)s")
    )
    logger.addHandler(log_handler)
    log_handler.setLevel(logging.INFO)
    logger.setLevel(logging.INFO)


class TQSClient(object):
    """
    TQS 查询客户端

    :param app_id: TQS 应用 ID
    :type app_id: Text
    :param app_key: TQS 应用 KEY
    :type app_key: Text
    :param cluster: TQS 集群名, 默认值: ``Cluster.CN``
    :type cluster: Text, optional
    :param max_retry: 网络请求失败后最大重试次数, 默认值: ``5``
    :type max_retry: int, optional
    :param retry_wait: 下一次重试前等待时间, 默认值: ``1`` 秒
    :type retry_wait: int, optional
    :param enable_domain: 是否开启域名访问 (连接失败时可尝试开启), 默认值: ``False``
    :type enable_domain: bool, optional
    :param timeout: 参考``requests.request``的``timeout``参数, 默认值: ``120`` 秒
    :type timeout: tuple, optional
    """

    def __init__(self, app_id, app_key, cluster=Cluster.CN, **kwargs):
        # type: (Text, Text, Text, Dict[Text, Any]) -> None
        assert cluster in CLUSTER_MAP, "invalid cluster: {}, choose from ({})".format(
            cluster, ",".join(CLUSTER_MAP.keys())
        )
        self.cluster = cluster
        self._cluster = CLUSTER_MAP[cluster]
        self.app_id = app_id
        self.app_key = app_key
        self.max_retry = kwargs.get("max_retry", 5)
        self.retry_wait = kwargs.get("retry_wait", 1)
        self.timeout = kwargs.get("timeout", 120)
        self.idc_name = bytedenv.get_idc_name()
        self._consul_lookup_addr_family = None
        self._consul_lookup_dual_stack_unique = None
        self.set_consul_lookup_addr_family("dual-stack")
        self.analyze_with_runtime_config = kwargs.get("analyze_with_runtime_config", False)
        self.local_ip, self.local_ip_error = get_local_ip()
        self.client_version, self.client_version_error = get_version_number()
        self.client_type = "bytedtqs"
        if "enable_domain" in kwargs:
            self.enable_domain = kwargs.get("enable_domain")
        elif self.idc_name in {"boe", "devbox"}:
            self.enable_domain = self.cluster != Cluster.BOE
        elif self.cluster == Cluster.BOE:
            self.enable_domain = True
        else:
            self.enable_domain = False
        logger.info("Create tqs client with cluster: {}, app id: {}, deploy idc: {},"
                    " timeout: {}s, client version: {}, local ip: {}".format(self.cluster, self.app_id,
                                                                             self._get_idc(), self.timeout,
                                                                             self.client_version, self.local_ip))

    def set_consul_lookup_addr_family(self, address_family):
        # type: (Text) -> None
        """
        Setting address family for consul lookup.
            v4: ipv4 only, v6: ipv6 only, dual-stack: prefer ipv6

        :param address_family: v4/v6/dual-stack, default ``None``
        """
        assert address_family in {"v4", "v6", "dual-stack"}, "invalid addr family, choose from v4/v6/dual-stack"
        is_client_support_ipv6 = len(os.environ.get("BYTED_HOST_IPV6", "")) > 0
        if address_family == "v6":
            assert is_client_support_ipv6, "client does not support ipv6, for 'BYTED_HOST_IPV6' not found in env"
        if address_family == "dual-stack":
            if not is_client_support_ipv6:
                self._consul_lookup_addr_family = "v4"
                return
            self._consul_lookup_dual_stack_unique = "v6"
        else:
            self._consul_lookup_dual_stack_unique = None
        self._consul_lookup_addr_family = address_family

    def submit_query(self, user_name, query, **kwargs):
        # type: (Text, Text, Dict[Text, Any]) -> int
        """
        提交查询任务，并立即返回

        :param user_name: 用户名
        :type user_name: Text
        :param query: 查询语句
        :type query: Text
        :param conf: 查询配置
        :type conf: dict, optional
        :param name: 任务名称
        :type name: Text, optional
        :param dry_run: 是否跳过SQL执行, 默认值：``False``
        :type dry_run: bool, optional
        :param skip_cost_analysis: 是否跳过解析 (super app only) , 默认值：``False``
        :type skip_cost_analysis: bool, optional
        :return: 任务ID
        :rtype: int
        """
        conf = kwargs.get("conf", dict())
        name = kwargs.get("name", "")
        dry_run = kwargs.get("dry_run", False)
        skip_cost_analysis = kwargs.get("skip_cost_analysis", False)
        query, query_conf = split_query_conf(query)
        conf.update(query_conf)
        data = {
            "user": user_name,
            "query": query,
            "dryRun": dry_run,
            "name": name,
            "conf": json.dumps({str(k): str(v) for k, v in conf.items()}),
            "skipCostAnalysis": skip_cost_analysis,
        }
        job_id = self._post_json("/api/v1/queries", json=data)["jobId"]
        logger.info("job submitted, job_id: {}".format(job_id))
        return job_id

    def execute_query(self, user_name, query, **kwargs):
        # type: (Text, Text, Dict[Text, Any]) -> JobEntity
        """
        执行查询任务，并等待其运行结束

        :param user_name: 用户名
        :type user_name: Text
        :param query: 查询语句
        :type query: Text
        :param kwargs: 参考 ``submit_query``
        :return: 任务描述
        :rtype: JobEntity
        """
        job_id = self.submit_query(user_name, query, **kwargs)
        job = self.wait_for_finish(job_id)
        return job

    def explain_query(self, user_name, query, **kwargs):
        # type: (Text, Text, Dict[Text, Any]) -> JobEntity
        """
        Alias of ``execute_query`` with ``dry_run=True``

        :param user_name: 用户名
        :type user_name: Text
        :param query: 查询语句
        :type query: Text
        :param kwargs: 参考 ``submit_query``
        :return: 任务描述
        :rtype: JobEntity
        """
        return self.execute_query(user_name, query, dry_run=True, **kwargs)

    def cancel_query(self, job_id):
        # type: (int) -> bool
        """
        取消查询任务

        :param job_id: 任务ID
        :type job_id: int
        :return: 是否取消成功
        :rtype: bool
        """
        return (
            self._request("post", "/api/v1/jobs/{}/cancel".format(job_id)).text == "OK"
        )

    def analyze_query(self, user_name, query):
        # type: (Text, Text) -> AnalysisResultEntity
        """
        执行SQL解析

        :param user_name: 用户名
        :type user_name: Text
        :param query: 查询语句
        :type query: Text
        :return: 解析结果
        :rtype: AnalysisResultEntity
        """
        data = {"user": user_name, "query": query, "useRuntimeConfig": self.analyze_with_runtime_config}
        return AnalysisResultEntity(
            self._post_json("/api/v1/queries/analysis", json=data)
        )

    def pass_stream_query(self, user_name, query):
        # type: (Text, Text) -> AnalysisResultEntity
        """
        批流一体执行流作业SQL解析

        :param user_name: 用户名
        :type user_name: Text
        :param query: 流sql查询语句
        :type query: Text
        :return: 解析结果
        :rtype: AnalysisResultEntity，返回实际符合规范的流作业文本
        """
        data = {"user": user_name, "streamQuery": query}
        print(data)
        return AnalysisResultEntity(
            self._post_json("/api/v1/queries/unified-sql/stream-sql", json=data)
        )

    def pass_batch_query(self, user_name, query):
        # type: (Text, Text) -> AnalysisResultEntity
        """
        批流一体执行执行批SQL解析

        :param user_name: 用户名
        :type user_name: Text
        :param query: 批sql查询语句
        :type query: Text
        :return: 解析结果
        :rtype: AnalysisResultEntity，返回实际符合规范的批作业文本
        """
        data = {"user": user_name, "batchQuery": query}
        print(data)
        return AnalysisResultEntity(
            self._post_json("/api/v1/queries/unified-sql/batch-sql", json=data)
        )

    def pass_table_query(self, user_name, streamQuery, batchQuery):
        # type: (Text, Text) -> AnalysisResultEntity
        """
        批流一体通过SQL解析对应的表

        :param user_name: 用户名
        :type user_name: Text
        :param streamQuery: 流sql
        :type streamQuery: Text
        :param batchQuery: 批sql
        :type batchQuery: Text
        :return: 解析结果
        :rtype: AnalysisResultEntity，对应的流批两个列表内容
        """
        data = {"user": user_name, "streamQuery": streamQuery, "batchQuery": batchQuery}
        print(data)
        return AnalysisResultEntity(
            self._post_json("/api/v1/queries/unified-sql/ddl-mapping", json=data)
        )

    def pass_datatrans_query(self, user_name, stream_ddls, batch_table_names):
        # type: (Text, Text) -> AnalysisResultEntity
        """
        批流一体执行生成流导数任务

        :param user_name: 用户名
        :type user_name Text
        :param stream_ddls: 流表ddl数组
        :type stream_ddls Text
        :param batch_table_names: 查询语句
        :type batch_table_names: Text
        :return: 解析结果
        :rtype: AnalysisResultEntity，返回结果内容中包含实际的导数作业文本
        """
        data = {"user": user_name, "streamDdls": stream_ddls,
                "batchTableNames": batch_table_names, "fromStream": True}
        print(data)
        return AnalysisResultEntity(
            self._post_json("/api/v1/queries/unified-sql/data-dump", json=data)
        )

    def get_job(self, job_id):
        # type: (int) -> JobEntity
        """
        获取任务描述

        :param job_id: 任务ID
        :type job_id: int
        :return: 任务描述
        :rtype: JobEntity
        """
        return JobEntity(
            self._get_json("/api/v1/jobs/%s" % job_id), self._get_query_result
        )

    def wait_for_finish(self, job_id):
        # type: (int) -> JobEntity
        """
        等待任务运行结束

        :param job_id: 任务ID
        :type job_id: int
        :return: 任务描述
        :rtype: JobEntity
        """
        start_time = time.time()
        while True:
            job = self.get_job(job_id)
            msg = "job_id: {}, engine_type: {}, status: {}".format(job_id, job.engine_type, job.status)
            if len(job.tracking_urls) > 0:
                msg += ", tracking_urls: {}".format(", ".join(job.tracking_urls))
            if job.extra_message and len(job.extra_message.strip()) != 0:
                msg += ", extra info: '{}'".format(job.extra_message)
            logger.info(msg)
            if job.is_finished():
                return job
            time.sleep(2 if time.time() - start_time < 60 else 5)

    def _get_query_result(self, job):
        # type: (JobEntity) -> QueryResultEntity
        """
        :type job: JobEntity
        :rtype: QueryResultEntity
        """
        assert job.is_success(), "result is not available for job status: {}".format(
            job.status
        )
        return QueryResultEntity(
            self._get_json("/api/v1/queries/{}".format(job.id)),
            job,
            self._csv_result_iterator,
        )

    def _csv_result_iterator(self, query_result):
        # type: (QueryResultEntity) -> Iterator[List[Text]]
        """
        :type query_result: QueryResultEntity
        :rtype: Iterator[List[Text]]
        """
        download_path = query_result.result_url[
            query_result.result_url.find("/download/results"):
        ]
        line_iterator = self._request("get", download_path, stream=True).iter_lines(
            decode_unicode=six.PY3
        )
        line_iterator = (line.replace(str("\0"), str("")) for line in line_iterator)
        csv_iterator = csv.reader(line_iterator, delimiter=str(","), quotechar=str('"'))

        def transform(iterator):
            for row in iterator:
                yield [field.decode("utf-8") for field in row]

        return csv_iterator if six.PY3 else transform(csv_iterator)

    def _get_cluster(self, path):
        # type: (Text) -> Cluster
        if self.cluster not in {Cluster.SG, Cluster.SG_PRIEST}:
            return self._cluster

        import re
        patterns = [
            r"/api/v1/jobs/(\d+)/cancel",
            r"/api/v1/jobs/(\d+)",
            r"/api/v1/queries/(\d+)",
            r"/download/results/\d+/\d+/.+/(\d+)_.+.csv",
        ]
        for pat in patterns:
            matches = re.findall(pat, path)
            if len(matches) == 1:
                job_id = int(matches[0])
                if self.cluster == Cluster.SG and job_id < 100000000:
                    return CLUSTER_MAP[Cluster.ALISG]
                elif self.cluster == Cluster.SG_PRIEST and job_id < 100000000:
                    return CLUSTER_MAP[Cluster.ALISG_PRIEST]
        return self._cluster

    def _choose_random_server(self, path=None):
        # type: (Text) -> Text
        """
        :type path: Text
        :rtype: Text
        """
        cluster = self._get_cluster(path)

        if self.enable_domain:
            return cluster.domain
        if path == "/api/v1/queries/analysis" and cluster.analysis_psm:
            psm = cluster.analysis_psm
        else:
            psm = cluster.api_psm
        all_server = []
        for rec in servicediscovery.lookup(psm, address_family=self._consul_lookup_addr_family,
                                           dualstack_unique=self._consul_lookup_dual_stack_unique):
            host, port = rec["Host"], str(rec["Port"])
            host = "[" + host + "]" if is_ipv6(host) else host
            all_server.append((host, port))
        if len(all_server) == 0:
            raise TQSEmptyConsulResultException(
                "consul discovery returns empty result, psm: {}".format(psm)
            )
        return ":".join(random.choice(all_server))

    def _request(self, method, path, **kwargs):
        # type: (Text, Text, Dict[Text, Any]) -> requests.Response
        """
        :type method: Text
        :type path: Text
        :rtype: requests.Response
        """
        try_count = 0
        headers = {
            "Content-Type": "application/json; charset=utf-8",
            "X-TQS-AppID": self.app_id,
            "X-TQS-AppKey": self.app_key,
            "x-tqs-client-ip": self.local_ip,
            "x-tqs-client-ip-error": self.local_ip_error,
            "x-tqs-client-version": self.client_version,
            "x-tqs-client-version-error": self.client_version_error,
            "x-tqs-client-type": self.client_type
        }
        url = path
        while True:
            try:
                try_count += 1
                server = self._choose_random_server(path)
                url = "http://{}{}".format(server, path)
                logger.debug("begin request, method: {}, url: {}".format(method, url))
                result = requests.request(
                    method, url, headers=headers, timeout=self.timeout, **kwargs
                )
                logger.debug(
                    "end request, url: {}, code: {}".format(url, result.status_code)
                )
                if result.status_code == 403:
                    raise TQSAuthException(
                        result.text
                    )
                result.raise_for_status()
                return result
            except TQSAuthException:
                raise
            except Exception as ex:
                if try_count > self.max_retry:
                    raise ex
                logger.warning(
                    "[retry {}/{}] request url: {}, reason: {} {}".format(
                        try_count, self.max_retry, url, type(ex), ex
                    )
                )
                time.sleep(self.retry_wait)

    def _post_json(self, path, **kwargs):
        # type: (Text, Dict[Text, Any]) -> Dict[Text, Any]
        """
        :type path: Text
        :rtype: dict
        """
        return self._request("post", path, **kwargs).json()

    def _get_json(self, path, **kwargs):
        # type: (Text, Dict[Text, Any]) -> Dict[Text, Any]
        """
        :type path: Text
        :rtype: dict
        """
        return self._request("get", path, **kwargs).json()

    # modified from pyutil/idc/idc.py
    def _parse_idc_name(self):
        try:
            idc_name = os.environ.get("RUNTIME_IDC_NAME").strip()
            if idc_name and idc_name != "unknown":
                return idc_name
        except: # noqa
            pass

        try:
            with open('/opt/tmp/consul_agent/datacenter', 'r') as f:
                idc_name = f.read().strip()
                if idc_name:
                    return idc_name
        except: # noqa
            pass

        try:
            idc_name = os.popen('/opt/tiger/consul_deploy/bin/determine_dc.sh 2>/dev/null').read().strip()
            if idc_name:
                return idc_name
        except: # noqa
            pass

        try:
            idc_name = os.popen(
                '/opt/tiger/ss_lib/bin/sd report 2>/dev/null |grep "Data center"|awk \'{print $3}\'').read().strip()
            if idc_name:
                return idc_name
        except: # noqa
            pass
        raise Exception("get idc failed")

    def _get_idc(self):
        try:
            return self._parse_idc_name().lower()
        except: # noqa
            return "unknown"

    def get_og_download_domain(self, domain):
        checkStarts = domain.startswith("http") or domain.startswith("https")
        checkEnds = domain.endswith("csv") or domain.endswith("log")
        if checkStarts and checkEnds:
            if self._cluster.og_domain_suffix and self._cluster.product_domain_suffix:
                return domain.replace(self._cluster.product_domain_suffix, self._cluster.og_domain_suffix)
            else:
                logger.warn("The cluster {} is not supported to get og domain".format(self.cluster))
        else:
            logger.warn("The domain {} provided is not supported and will return origin domain".format(domain))
        return domain
